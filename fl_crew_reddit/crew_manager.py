"""
Manager/Orchestrateur du FL Crew Reddit
Coordonne les 5 agents pour le Federated Learning
Fichier: fl_crew_reddit/crew_manager.py
"""

import asyncio
from typing import Dict, List, Any, Optional
from pathlib import Path
import json
from datetime import datetime

from crewai import Crew, Process, Task
from crewai.agent import Agent

from .config import FLCrewRedditConfig
from .mcp_client import RedditMCPClient
from .tools.mcp_tools import MCPRedditTools
from .tools.data_tools import DataProcessingTools
from .tools.model_tools import ModelTrainingTools
from .agents import (
    DataCollectorAgent,
    DataPreprocessorAgent,
    ModelTrainerAgent,
    AggregatorAgent,
    EvaluatorAgent
)


class FLCrewRedditManager:
    """Manager pour orchestrer le FL Crew Reddit avec 5 agents"""
    
    def __init__(self, config: FLCrewRedditConfig = None):
        """
        Initialise le manager du FL Crew
        
        Args:
            config: Configuration du FL Crew (optionnel)
        """
        self.config = config or FLCrewRedditConfig()
        self.config.validate()
        
        # Initialiser les clients et outils
        self.mcp_client = RedditMCPClient()
        self.mcp_tools = MCPRedditTools(self.mcp_client)
        self.data_tools = DataProcessingTools(self.config.PREPROCESSED_DATA_DIR)
        self.model_tools = ModelTrainingTools(
            self.config.MODELS_DIR,
            base_model=self.config.BASE_LLM_MODEL
        )
        
        # Initialiser les agents
        self.data_collector = DataCollectorAgent(self.mcp_tools)
        self.data_preprocessor = DataPreprocessorAgent(self.data_tools)
        
        # Cr√©er 3 agents Model Trainer (on peut en avoir plus si besoin)
        self.model_trainers = [
            ModelTrainerAgent(self.model_tools, agent_id=i+1)
            for i in range(3)  # 3 agents trainers pour avoir 5 agents au total
        ]
        
        self.aggregator = AggregatorAgent(self.model_tools)
        self.evaluator = EvaluatorAgent(self.model_tools)
        
        # Liste de tous les agents
        self.all_agents = [
            self.data_collector.get_agent(),
            self.data_preprocessor.get_agent(),
            *[trainer.get_agent() for trainer in self.model_trainers],
            self.aggregator.get_agent(),
            self.evaluator.get_agent()
        ]
        
        print(f"‚úÖ FL Crew Reddit Manager initialis√© avec {len(self.all_agents)} agents")
    
    def create_collection_task(self) -> Task:
        """Cr√©e la t√¢che de collecte de donn√©es"""
        return Task(
            description="""
            Collecte des donn√©es Reddit depuis le MCP Server pour le Federated Learning.
            
            Tu dois:
            1. Collecter des posts depuis plusieurs subreddits pertinents
            2. Collecter des commentaires associ√©s
            3. Collecter des informations sur les utilisateurs
            4. Sauvegarder toutes les donn√©es collect√©es
            
            Subreddits sugg√©r√©s: python, machinelearning, datascience, artificial, technology
            Limite par subreddit: 50-100 posts
            """,
            agent=self.data_collector.get_agent(),
            expected_output="Rapport JSON avec les donn√©es collect√©es: nombre de posts, commentaires, utilisateurs collect√©s"
        )
    
    def create_preprocessing_task(self) -> Task:
        """Cr√©e la t√¢che de pr√©traitement"""
        return Task(
            description="""
            Pr√©pare les donn√©es collect√©es pour l'entra√Ænement.
            
            Tu dois:
            1. Nettoyer et normaliser les donn√©es
            2. Extraire les features pertinentes
            3. Diviser les donn√©es en ensembles d'entra√Ænement et de test
            4. Pr√©parer les donn√©es pour chaque agent trainer
            
            Assure-toi que les donn√©es sont bien structur√©es et pr√™tes pour l'entra√Ænement.
            """,
            agent=self.data_preprocessor.get_agent(),
            expected_output="Rapport JSON avec les donn√©es pr√©par√©es: nombre d'√©chantillons, features extraites, division train/test"
        )
    
    def create_training_task(self, agent_id: int) -> Task:
        """Cr√©e une t√¢che d'entra√Ænement LLM pour un agent"""
        trainer = self.model_trainers[agent_id - 1]
        return Task(
            description=f"""
            Entra√Æne un mod√®le LLM local avec fine-tuning (LoRA) pour l'agent {agent_id}.
            
            Tu dois:
            1. Charger les donn√©es d'entra√Ænement multimodales pr√©par√©es (texte, images)
            2. Entra√Æner un mod√®le LLM local avec fine-tuning LoRA sur {self.config.LOCAL_EPOCHS} √©poques
            3. Utiliser le mod√®le de base: {self.config.BASE_LLM_MODEL}
            4. Sauvegarder le mod√®le LoRA entra√Æn√©
            5. Fournir des m√©triques d'entra√Ænement (perplexity, loss, etc.)
            
            Les donn√©es sont multimodales (texte + images), assure-toi de bien les traiter.
            Optimise les hyperparam√®tres LoRA si n√©cessaire (lora_r, learning_rate).
            """,
            agent=trainer.get_agent(),
            expected_output=f"Rapport JSON avec le mod√®le LLM entra√Æn√© de l'agent {agent_id}: chemin du mod√®le LoRA, m√©triques d'entra√Ænement (perplexity, loss)"
        )
    
    def create_aggregation_task(self, round_number: int) -> Task:
        """Cr√©e la t√¢che d'agr√©gation LLM"""
        return Task(
            description=f"""
            Agr√®ge les mod√®les LLM locaux (LoRA) des agents en un mod√®le global pour le round {round_number}.
            
            Tu dois:
            1. R√©cup√©rer tous les mod√®les LoRA locaux entra√Æn√©s par les agents
            2. Appliquer Federated Averaging sur les poids LoRA pour cr√©er un mod√®le agr√©g√©
            3. M√©thode d'agr√©gation: {self.config.AGGREGATION_METHOD}
            4. Sauvegarder le mod√®le LoRA agr√©g√©
            5. Pr√©parer le mod√®le agr√©g√© pour la distribution aux agents (pour le prochain round)
            
            Assure-toi que l'agr√©gation des poids LoRA est correcte et √©quitable.
            Le mod√®le de base reste: {self.config.BASE_LLM_MODEL}
            """,
            agent=self.aggregator.get_agent(),
            expected_output=f"Rapport JSON avec le mod√®le LLM agr√©g√© du round {round_number}: chemin du mod√®le LoRA agr√©g√©, nombre de mod√®les agr√©g√©s, m√©triques d'agr√©gation"
        )
    
    def create_evaluation_task(self, model_type: str = "aggregated") -> Task:
        """Cr√©e la t√¢che d'√©valuation LLM"""
        return Task(
            description=f"""
            √âvalue les performances du mod√®le LLM {model_type}.
            
            Tu dois:
            1. Charger le mod√®le LLM √† √©valuer (LoRA ou agr√©g√©)
            2. Charger les donn√©es de test multimodales
            3. Calculer les m√©triques LLM appropri√©es:
               - Perplexity (pour g√©n√©ration de texte)
               - BLEU/ROUGE scores (qualit√© de g√©n√©ration)
               - Accuracy/F1 (si classification)
               - M√©triques multimodales (image-text alignment si applicable)
            4. G√©n√©rer un rapport d'√©valuation d√©taill√©
            
            Les donn√©es sont multimodales, √©value aussi la performance cross-modale si applicable.
            Fournis des recommandations d'am√©lioration si n√©cessaire.
            """,
            agent=self.evaluator.get_agent(),
            expected_output="Rapport JSON avec les m√©triques d'√©valuation LLM: perplexity, BLEU, ROUGE, accuracy, m√©triques multimodales, recommandations"
        )
    
    def run_federated_learning_round(self, round_number: int) -> Dict[str, Any]:
        """
        Ex√©cute un round complet de Federated Learning
        
        Args:
            round_number: Num√©ro du round
            
        Returns:
            R√©sultats du round
        """
        print(f"\n{'='*60}")
        print(f"üîÑ Round {round_number} de Federated Learning")
        print(f"{'='*60}\n")
        
        results = {
            "round": round_number,
            "timestamp": datetime.now().isoformat(),
            "status": "in_progress"
        }
        
        try:
            # 1. Collecte de donn√©es (seulement au premier round)
            if round_number == 1:
                print("üìä √âtape 1: Collecte de donn√©es...")
                collection_task = self.create_collection_task()
                collection_crew = Crew(
                    agents=[self.data_collector.get_agent()],
                    tasks=[collection_task],
                    process=Process.sequential,
                    verbose=True
                )
                collection_result = collection_crew.kickoff()
                results["collection"] = str(collection_result)
            
            # 2. Pr√©traitement (seulement au premier round)
            if round_number == 1:
                print("\nüîß √âtape 2: Pr√©traitement des donn√©es...")
                preprocessing_task = self.create_preprocessing_task()
                preprocessing_crew = Crew(
                    agents=[self.data_preprocessor.get_agent()],
                    tasks=[preprocessing_task],
                    process=Process.sequential,
                    verbose=True
                )
                preprocessing_result = preprocessing_crew.kickoff()
                results["preprocessing"] = str(preprocessing_result)
            
            # 3. Entra√Ænement local (chaque round)
            print(f"\nüèãÔ∏è √âtape 3: Entra√Ænement local des agents...")
            training_tasks = [
                self.create_training_task(agent_id=i+1)
                for i in range(len(self.model_trainers))
            ]
            
            training_crew = Crew(
                agents=[trainer.get_agent() for trainer in self.model_trainers],
                tasks=training_tasks,
                process=Process.sequential,
                verbose=True
            )
            training_result = training_crew.kickoff()
            results["training"] = str(training_result)
            
            # 4. Agr√©gation (chaque round)
            print(f"\nüîó √âtape 4: Agr√©gation des mod√®les...")
            aggregation_task = self.create_aggregation_task(round_number)
            aggregation_crew = Crew(
                agents=[self.aggregator.get_agent()],
                tasks=[aggregation_task],
                process=Process.sequential,
                verbose=True
            )
            aggregation_result = aggregation_crew.kickoff()
            results["aggregation"] = str(aggregation_result)
            
            # 5. √âvaluation (chaque round)
            print(f"\nüìà √âtape 5: √âvaluation du mod√®le...")
            evaluation_task = self.create_evaluation_task("aggregated")
            evaluation_crew = Crew(
                agents=[self.evaluator.get_agent()],
                tasks=[evaluation_task],
                process=Process.sequential,
                verbose=True
            )
            evaluation_result = evaluation_crew.kickoff()
            results["evaluation"] = str(evaluation_result)
            
            results["status"] = "completed"
            print(f"\n‚úÖ Round {round_number} termin√© avec succ√®s!")
            
        except Exception as e:
            results["status"] = "error"
            results["error"] = str(e)
            print(f"\n‚ùå Erreur lors du round {round_number}: {e}")
        
        return results
    
    def run_federated_learning(self, num_rounds: int = None) -> Dict[str, Any]:
        """
        Ex√©cute le processus complet de Federated Learning
        
        Args:
            num_rounds: Nombre de rounds (par d√©faut depuis la config)
            
        Returns:
            R√©sultats complets du processus
        """
        num_rounds = num_rounds or self.config.FEDERATION_ROUNDS
        
        print("\n" + "="*60)
        print("üöÄ D√âMARRAGE DU FEDERATED LEARNING - FL CREW REDDIT")
        print("="*60)
        print(f"üìã Configuration:")
        print(f"   - Nombre d'agents: {len(self.all_agents)}")
        print(f"   - Nombre de rounds: {num_rounds}")
        print(f"   - Mod√®le de base LLM: {self.config.BASE_LLM_MODEL}")
        print(f"   - Fine-tuning: LoRA (r={self.config.LORA_R})")
        print(f"   - √âpoques locales: {self.config.LOCAL_EPOCHS}")
        print(f"   - Batch size: {self.config.BATCH_SIZE}")
        print(f"   - Learning rate: {self.config.LEARNING_RATE}")
        print(f"   - Donn√©es multimodales: {self.config.SUPPORT_MULTIMODAL}")
        print(f"   - M√©thode d'agr√©gation: {self.config.AGGREGATION_METHOD}")
        print("="*60 + "\n")
        
        all_results = {
            "start_time": datetime.now().isoformat(),
            "config": {
                "num_agents": len(self.all_agents),
                "num_rounds": num_rounds,
                "base_llm_model": self.config.BASE_LLM_MODEL,
                "use_lora": self.config.USE_LORA,
                "lora_r": self.config.LORA_R,
                "local_epochs": self.config.LOCAL_EPOCHS,
                "batch_size": self.config.BATCH_SIZE,
                "learning_rate": self.config.LEARNING_RATE,
                "support_multimodal": self.config.SUPPORT_MULTIMODAL,
                "aggregation_method": self.config.AGGREGATION_METHOD
            },
            "rounds": []
        }
        
        for round_num in range(1, num_rounds + 1):
            round_results = self.run_federated_learning_round(round_num)
            all_results["rounds"].append(round_results)
            
            # Sauvegarder les r√©sultats apr√®s chaque round
            self._save_results(all_results, round_num)
        
        all_results["end_time"] = datetime.now().isoformat()
        all_results["status"] = "completed"
        
        # Sauvegarder les r√©sultats finaux
        self._save_results(all_results, "final")
        
        print("\n" + "="*60)
        print("üéâ FEDERATED LEARNING TERMIN√â!")
        print("="*60)
        print(f"‚úÖ {num_rounds} rounds compl√©t√©s")
        print(f"üìÅ R√©sultats sauvegard√©s dans: {self.config.RESULTS_DIR}")
        print("="*60 + "\n")
        
        return all_results
    
    def _save_results(self, results: Dict[str, Any], identifier: str):
        """Sauvegarde les r√©sultats dans un fichier JSON"""
        results_file = self.config.RESULTS_DIR / f"fl_results_{identifier}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"üíæ R√©sultats sauvegard√©s: {results_file}")

