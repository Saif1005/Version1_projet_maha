{
  "id": "1nxb9bp",
  "title": "[R] New paper shows that draws in LLM battles aren't what you think",
  "selftext": "Arena evals (e.g., Chatbot Arena) let users pick which model's response is better, or call it a draw. Most leaderboards then shove this into Elo, same as chess. The assumption: a draw = two models are equally strong. The paper [\"Drawing Conclusions from Draws: Rethinking Preference Semantics in Arena-Style LLM Evaluation\"](https://arxiv.org/abs/2510.02306) tests that assumption and proves it wrong:\n\n* On 3 arena datasets, ignoring draws when updating ratings makes battle outcome prediction accuracy go **up 1-3%**, despite evaluation still *including draws*.\n* Draws happen much more on **easy** or **objective** queries (risk ratios of 1.3x).\n\n**Discussion seed:** If draws don't indicate skill parity and hence represent a poor fit for existing rating systems, how should we *actually* model them?\n\nCOI: Submitter is author.",
  "author": "tetrisdaemon",
  "subreddit": "MachineLearning",
  "created_utc": "2025-10-03T22:08:42",
  "score": 18,
  "upvote_ratio": 0.77,
  "num_comments": 10,
  "permalink": "/r/MachineLearning/comments/1nxb9bp/r_new_paper_shows_that_draws_in_llm_battles_arent/",
  "url": "https://www.reddit.com/r/MachineLearning/comments/1nxb9bp/r_new_paper_shows_that_draws_in_llm_battles_arent/",
  "is_self": true,
  "is_video": false,
  "over_18": false,
  "link_flair_text": "Research",
  "retrieved_at": "2025-10-04T12:13:30.257080"
}