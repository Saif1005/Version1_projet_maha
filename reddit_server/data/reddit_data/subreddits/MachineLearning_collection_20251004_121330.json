{
  "subreddit": "MachineLearning",
  "posts": [
    {
      "id": "1nxfyl8",
      "title": "[D] join pretraining or posttraining",
      "selftext": "Hello!\n\nI have the possibility to join one of the few AI lab that trains their own LLMs.\n\nGiven the option, would you join the pretraining team or (core) post training team? Why so?",
      "author": "oxydis",
      "subreddit": "MachineLearning",
      "created_utc": "2025-10-04T01:33:52",
      "score": 26,
      "upvote_ratio": 0.93,
      "num_comments": 11,
      "permalink": "/r/MachineLearning/comments/1nxfyl8/d_join_pretraining_or_posttraining/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1nxfyl8/d_join_pretraining_or_posttraining/",
      "is_self": true,
      "is_video": false,
      "over_18": false,
      "link_flair_text": "Discussion",
      "retrieved_at": "2025-10-04T12:13:30.257075"
    },
    {
      "id": "1nxb9bp",
      "title": "[R] New paper shows that draws in LLM battles aren't what you think",
      "selftext": "Arena evals (e.g., Chatbot Arena) let users pick which model's response is better, or call it a draw. Most leaderboards then shove this into Elo, same as chess. The assumption: a draw = two models are equally strong. The paper [\"Drawing Conclusions from Draws: Rethinking Preference Semantics in Arena-Style LLM Evaluation\"](https://arxiv.org/abs/2510.02306) tests that assumption and proves it wrong:\n\n* On 3 arena datasets, ignoring draws when updating ratings makes battle outcome prediction accuracy go **up 1-3%**, despite evaluation still *including draws*.\n* Draws happen much more on **easy** or **objective** queries (risk ratios of 1.3x).\n\n**Discussion seed:** If draws don't indicate skill parity and hence represent a poor fit for existing rating systems, how should we *actually* model them?\n\nCOI: Submitter is author.",
      "author": "tetrisdaemon",
      "subreddit": "MachineLearning",
      "created_utc": "2025-10-03T22:08:42",
      "score": 18,
      "upvote_ratio": 0.77,
      "num_comments": 10,
      "permalink": "/r/MachineLearning/comments/1nxb9bp/r_new_paper_shows_that_draws_in_llm_battles_arent/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1nxb9bp/r_new_paper_shows_that_draws_in_llm_battles_arent/",
      "is_self": true,
      "is_video": false,
      "over_18": false,
      "link_flair_text": "Research",
      "retrieved_at": "2025-10-04T12:13:30.257080"
    },
    {
      "id": "1nwwsk7",
      "title": "[P] I am building a ML job board",
      "selftext": "Hey fellow ML people!\n\nLast year, I shared with you a job board for [FAANG positions](https://www.reddit.com/r/MachineLearning/comments/1ia7feh/p_made_a_faang_job_postings_aggregator_for_ai/) and due to the positive feedback I received, I had been working on expanded version called [hire.watch](https://hire.watch/?categories=AI+_+Machine+Learning)\n\nThe goal is provide a unified search experience - it crawls, cleans and extracts data, allowing filtering by:\n\n1. Full-text search\n2. Location - on-site\n3. Remote - from a given city, US state, EU, etc.\n4. Category - you can check out the machine learning category here: [https://hire.watch/?categories=AI+\\_+Machine+Learning](https://hire.watch/?categories=AI+_+Machine+Learning)\n5. Years of experience and seniority\n6. Target gross salary\n7. Date posted and date modified\n\nI used the normal ML ecosystem (scikit learn, huggingface transformers, LLMs, etc.) to build it, and Plotly Dash for the UI.\n\nLet me know what you think - feel free to ask questions and request features :)",
      "author": "dev-ai",
      "subreddit": "MachineLearning",
      "created_utc": "2025-10-03T12:42:47",
      "score": 7,
      "upvote_ratio": 0.61,
      "num_comments": 0,
      "permalink": "/r/MachineLearning/comments/1nwwsk7/p_i_am_building_a_ml_job_board/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1nwwsk7/p_i_am_building_a_ml_job_board/",
      "is_self": true,
      "is_video": false,
      "over_18": false,
      "link_flair_text": "Project",
      "retrieved_at": "2025-10-04T12:13:30.257083"
    },
    {
      "id": "1nwoxqz",
      "title": "[R] New paper: LLMs don't have privileged self knowledge, which means we can efficiently train a General Correctness Model to predict the correctness of multiple models. Surprising or expected?",
      "selftext": "Quick paper highlight (adapted from TLDR thread):  \nFinds no special advantage using an LLM to predict its own correctness (a trend in prior work), instead finding that LLMs benefit from learning to predict the correctness of many other models – becoming a GCM.  \n\\--  \nTraining 1 GCM is strictly more accurate than training model-specific CMs for all models it trains on (including CMs trained to predict their own correctness).  \nGCM transfers without training to outperform direct training on OOD models and datasets.  \nGCM (based on Qwen3-8B) achieves +30% coverage on selective prediction vs much larger Llama-3-70B’s logits.\n\nTLDR thread: [https://x.com/hanqi\\_xiao/status/1973088476691042527](https://x.com/hanqi_xiao/status/1973088476691042527)  \nFull paper: [https://arxiv.org/html/2509.24988v1](https://arxiv.org/html/2509.24988v1)\n\n**Discussion Seed**:  \nPrevious works have suggested / used LLMs having self knowledge, e.g., identifying/preferring their own generations \\[https://arxiv.org/abs/2404.13076\\], or ability to predict their uncertainty. But paper claims specifically that LLMs don't have knowledge about their own *correctness.* Curious on everyone's intuition for what LLMs have / does not have self knowledge about, and whether this result fit your predictions.\n\nConflict of Interest:   \nAuthor is making this post. ",
      "author": "Envoy-Insc",
      "subreddit": "MachineLearning",
      "created_utc": "2025-10-03T04:52:19",
      "score": 21,
      "upvote_ratio": 0.75,
      "num_comments": 10,
      "permalink": "/r/MachineLearning/comments/1nwoxqz/r_new_paper_llms_dont_have_privileged_self/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1nwoxqz/r_new_paper_llms_dont_have_privileged_self/",
      "is_self": true,
      "is_video": false,
      "over_18": false,
      "link_flair_text": "Research",
      "retrieved_at": "2025-10-04T12:13:30.257086"
    },
    {
      "id": "1nwhihj",
      "title": "[N] Stanford is updating their Deep Learning course on YouTube",
      "selftext": "This is a [great opportunity](https://www.youtube.com/watch?v=_NLHFoVNlbg) for all ML/DL students/practitioners to either start learning from scratch or filling knowledge gap, time to start learning folks.",
      "author": "al3arabcoreleone",
      "subreddit": "MachineLearning",
      "created_utc": "2025-10-02T23:03:22",
      "score": 179,
      "upvote_ratio": 0.96,
      "num_comments": 11,
      "permalink": "/r/MachineLearning/comments/1nwhihj/n_stanford_is_updating_their_deep_learning_course/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1nwhihj/n_stanford_is_updating_their_deep_learning_course/",
      "is_self": true,
      "is_video": false,
      "over_18": false,
      "link_flair_text": "News",
      "retrieved_at": "2025-10-04T12:13:30.257089"
    },
    {
      "id": "1nwfn4j",
      "title": "[R] Thesis direction: mechanistic interpretability vs semantic probing of LLM reasoning?",
      "selftext": "Hi all,\n\nI'm an undergrad Computer Science student working or my senior thesis, and l'll have about 8 months to dedicate to it nearly full-time. My broad interest is in reasoning, and I'm trying to decide between two directions:\n\n• Mechanistic interpretability (low-level): reverse engineering smaller neural networks, analyzing weights/ activations, simple logic gates, and tracking learning dynamics.\n\n•Semantic probing (high-level): designing behavioral tasks for LLMs, probing reasoning, attention/locality, and consistency of inference.\n\nFor context, after graduation I'll be joining a GenAl team as a software engineer. The role will likely lean more full-stack/frontend at first, but my long-term goal is to transition into backend.\n\nI'd like the thesis to be rigorous but also build skills that will be useful for my long-term goal of becoming a software engineer. From your perspective, which path might be more valuable in terms that of feasibility, skill development, and career impact?\n\nThanks in advance for your advice!",
      "author": "powerpuff___",
      "subreddit": "MachineLearning",
      "created_utc": "2025-10-02T21:50:44",
      "score": 11,
      "upvote_ratio": 0.79,
      "num_comments": 13,
      "permalink": "/r/MachineLearning/comments/1nwfn4j/r_thesis_direction_mechanistic_interpretability/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1nwfn4j/r_thesis_direction_mechanistic_interpretability/",
      "is_self": true,
      "is_video": false,
      "over_18": false,
      "link_flair_text": "Research",
      "retrieved_at": "2025-10-04T12:13:30.257091"
    },
    {
      "id": "1nwaunk",
      "title": "[D] How much should researchers (especially in ML domain) rely on LLMs for their work?",
      "selftext": "Are ML researchers using LLMs like ChatGPT, Claude, or other open-source models to generate, test, or refine minor ideas as tweaks to their original research, or to ask big-picture questions about their overall plans? In what other ways are publishing researchers using LLMs to support their work? (Of course, I don’t mean those who literally ask ChatGPT to write a paper from scratch.)\n\nI sometimes feel guilty when I feed a paper into ChatGPT and ask it to summarize or even extract “ideas” from it, which I then try to combine with my own. I want to understand where a researcher should draw the line in using LLMs in their daily workflow, so as not to fool themselves into believing they are doing good research while over-relying on the tool.",
      "author": "etoipi1",
      "subreddit": "MachineLearning",
      "created_utc": "2025-10-02T18:53:20",
      "score": 34,
      "upvote_ratio": 0.8,
      "num_comments": 53,
      "permalink": "/r/MachineLearning/comments/1nwaunk/d_how_much_should_researchers_especially_in_ml/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1nwaunk/d_how_much_should_researchers_especially_in_ml/",
      "is_self": true,
      "is_video": false,
      "over_18": false,
      "link_flair_text": "Discussion",
      "retrieved_at": "2025-10-04T12:13:30.257093"
    },
    {
      "id": "1nwanih",
      "title": "[D] Multi-market retail dataset for computer vision - 1M images, temporally organised by year",
      "selftext": "Hello all. I am sharing details about a retail focused dataset we've assembled that might interest folks working on production CV systems:\n\n**Quick specs:**\n\n* 1M retail interior images (280K structured, 720K available for processing) but all are structured and organised. 280k are our platinum set.\n* Multi-country: UK, US, Netherlands, Ireland, Germany. Mainly UK/US.\n* Temporal organisation: Year/month categorization spanning multiple years, also by retailer and week too.\n* Hierarchical structure: Year > Season > Retailer > Sub-Category (event specific) and often by month and week for Christmas.\n* Real-world conditions: Various lighting, angles, store formats.\n* Perfectly imperfect world of retail, all images taken for our consulting work, so each image has a story, good, bad, indifferent. \n\n**Why this might matter:** Most retail CV benchmarks (SKU110K, RP2K, etc.) are single market or synthetic. Real deployment requires models that handle:\n\n* Cross-retailer variation (Tesco ≠ Walmart ≠ Sainsburys et al)\n* Temporal shifts (seasonal merchandising, promotional displays, COVID we have too)\n* Geographic differences (EU vs US labeling, store formats)\n\n**Research applications:**\n\n* Domain adaptation across retail environments\n* Few shot learning for new product categories\n* Temporal consistency in object detection\n* Transfer learning benchmarks\n* Dates on product, reduction labels, out of stock, lows, highs.\n\n**Commercial applications:**\n\n* Training production planogram compliance systems\n* Autonomous checkout model training\n* Inventory management CV pipelines\n* Retail execution monitoring\n* Numerous other examples that could be developerd.\n\nAvailable for licensing (commercial) and academic partnerships. Can provide samples and detailed breakdown under NDA with a controlled sample available. \n\nCurious about the community's thoughts on what annotations would add most value - we can support custom categorisation and labelling work. \n\nIt's a new world for us in terms of licensing, we are retailers at heart but we know that 1m images from 2010 to today represents a really unique dataset.",
      "author": "malctucker",
      "subreddit": "MachineLearning",
      "created_utc": "2025-10-02T18:46:18",
      "score": 0,
      "upvote_ratio": 0.42,
      "num_comments": 0,
      "permalink": "/r/MachineLearning/comments/1nwanih/d_multimarket_retail_dataset_for_computer_vision/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1nwanih/d_multimarket_retail_dataset_for_computer_vision/",
      "is_self": true,
      "is_video": false,
      "over_18": false,
      "link_flair_text": "Project",
      "retrieved_at": "2025-10-04T12:13:30.257095"
    },
    {
      "id": "1nw8ql3",
      "title": "[D] Will fine-tuning LLaMA 3.2 11B Instruct on text-only data degrade its vision capabilities?",
      "selftext": "I'm planning to fine-tune LLaMA 3.2 11B Instruct on a JSONL dataset of domain-specific question-answer pairs — purely text, no images. The goal is to improve its instruction-following behavior for specialized text tasks, while still retaining its ability to handle multimodal inputs like OCR and image-based queries.\n\nMy concern: will this fine-tuning lead to multimodal forgetting?\n\nThe NeurIPS 2024 paper discusses how training on more image-text pairs can cause text-only forgetting. So I’m wondering — does the reverse happen too? If I train only on text, will the model lose its ability to process images or degrade in tasks like OCR?\n\nHas anyone observed this kind of modality drift or tested the impact of unimodal fine-tuning on multimodal performance?",
      "author": "PravalPattam12945RPG",
      "subreddit": "MachineLearning",
      "created_utc": "2025-10-02T17:35:46",
      "score": 7,
      "upvote_ratio": 0.89,
      "num_comments": 7,
      "permalink": "/r/MachineLearning/comments/1nw8ql3/d_will_finetuning_llama_32_11b_instruct_on/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1nw8ql3/d_will_finetuning_llama_32_11b_instruct_on/",
      "is_self": true,
      "is_video": false,
      "over_18": false,
      "link_flair_text": "Discussion",
      "retrieved_at": "2025-10-04T12:13:30.257109"
    },
    {
      "id": "1nw6jqf",
      "title": "[R] Maths PhD student - Had an idea on diffusion",
      "selftext": "I am a PhD student in Maths - high dimensional modeling. I had an idea for a future project, although since I am not too familiar with these concept, I would like to ask people who are, if I am thinking about this right and what your feedback is. \n\nTake diffusion for image generation. An overly simplified tldr description of what I understand is going on is this. Given pairs of (text, image) in the training set, the diffusion algorithm learns to predict the noise that was added to the image. It then creates a distribution of image concepts in a latent space so that it can generalize better. For example, let's say we had two concepts of images in our training set. One is of dogs eating ice cream and one is of parrots skateboarding. If during inference we asked the model to output a dog skateboarding, it would go to the latent space and sample an image which is somewhere \"in the middle\" of dogs eating ice cream and parrots skateboarding. And that image would be generated starting from random noise. \n\nSo my question is, can diffusion be used in the following way? Let's say I want the algorithm to output a vector of numbers (p) given an input vector of numbers (x), where this vector p would perform well based on a criterion I select. So the approach I am thinking is to first generate pairs of (x, p) for training, by generating \"random\" (or in some other way) vectors p, evaluating them and then keeping the best vectors as pairs with x. Then I would train the diffusion algorithm as usual. Finally, when I give the trained model a new vector x, it would be able to output a vector p which performs well given x. \n\n  \nPlease let me know if I have any mistakes in my thought process or if you think that would work in general. Thank you.",
      "author": "5000marios",
      "subreddit": "MachineLearning",
      "created_utc": "2025-10-02T16:14:33",
      "score": 24,
      "upvote_ratio": 0.74,
      "num_comments": 47,
      "permalink": "/r/MachineLearning/comments/1nw6jqf/r_maths_phd_student_had_an_idea_on_diffusion/",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1nw6jqf/r_maths_phd_student_had_an_idea_on_diffusion/",
      "is_self": true,
      "is_video": false,
      "over_18": false,
      "link_flair_text": "Research",
      "retrieved_at": "2025-10-04T12:13:30.257114"
    }
  ],
  "count": 10
}